import logging
import os
import numpy as np
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=512
EPOCHS=100
LR=0.01
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.4
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')

import numpy as np
np.random.seed(SEED)

import random
random.seed(SEED)

import logging
import os
import numpy as np
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=512
EPOCHS=100
LR=0.01
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.4
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')

import numpy as np
np.random.seed(SEED)

import random
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
epoch 1/100
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=100
LR=0.01
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.4
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=100
LR=0.01
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.4
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
epoch 1/100
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=128
EPOCHS=100
LR=0.01
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.4
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=128
EPOCHS=100
LR=0.01
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.4
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
epoch 1/100
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=100
LR=0.01
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.4
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=100
LR=0.01
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.4
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
epoch 1/100
loss=9.61101245880127 train_score=0.061310839317914784 val_score=0.06134368033695318
iter/secs=0.038461538461538464   lr=0.01
loss=3.047497272491455 train_score=0.6211363402145057 val_score=0.6081434504993517
iter/secs=2.5051546391752577   lr=0.01
loss=1.329182744026184 train_score=0.7575063590533994 val_score=0.7507310641417697
iter/secs=2.5759717314487633   lr=0.01
loss=1.0533244609832764 train_score=0.8578175632419223 val_score=0.8494589875741622
iter/secs=2.5851063829787235   lr=0.01
epoch 2/100
loss=0.9708737134933472 train_score=0.8288769927626425 val_score=0.8248860626403745
iter/secs=0.04   lr=0.01
loss=0.7161272168159485 train_score=0.881051966431989 val_score=0.8763427246996298
iter/secs=2.5943060498220643   lr=0.01
loss=0.6096264123916626 train_score=0.8943831662851631 val_score=0.8810302484474347
iter/secs=2.5851063829787235   lr=0.01
loss=0.6545356512069702 train_score=0.8749411642091451 val_score=0.8622602168340724
iter/secs=2.5943060498220643   lr=0.01
epoch 3/100
loss=0.465638667345047 train_score=0.9021265723557796 val_score=0.8933201697596856
iter/secs=0.04   lr=0.01
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.01
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.4
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.01
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.4
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
epoch 1/10
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.01
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.4
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.01
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.4
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
epoch 1/10
loss=9.689833641052246 train_score=0.06420005633211637 val_score=0.06443772364579763
iter/secs=0.3333333333333333   lr=0.01
loss=4.417614936828613 train_score=0.47634320054466955 val_score=0.47936896016423586
iter/secs=18.692307692307693   lr=0.01
loss=3.017244815826416 train_score=0.7164770093503564 val_score=0.7018313449314331
iter/secs=18.225   lr=0.01
loss=1.795200228691101 train_score=0.7534739131200756 val_score=0.7484196567846031
iter/secs=18.225   lr=0.01
epoch 2/10
loss=1.78812837600708 train_score=0.7839875410548707 val_score=0.7826766242293438
iter/secs=0.3333333333333333   lr=0.01
loss=1.8346855640411377 train_score=0.8189039547057411 val_score=0.8179096081863872
iter/secs=18.225   lr=0.01
loss=1.789289116859436 train_score=0.8539733073206044 val_score=0.840623956584712
iter/secs=18.225   lr=0.01
loss=1.3964078426361084 train_score=0.8536511592608584 val_score=0.8392695597982296
iter/secs=18.225   lr=0.01
epoch 3/10
loss=0.92162024974823 train_score=0.8596052856447993 val_score=0.8381668581875372
iter/secs=0.3333333333333333   lr=0.01
loss=1.381710171699524 train_score=0.8761601932937368 val_score=0.861197294859121
iter/secs=18.225   lr=0.01
loss=1.3089401721954346 train_score=0.8754035764330652 val_score=0.8711704753916176
iter/secs=17.78048780487805   lr=0.01
loss=2.4146032333374023 train_score=0.8911120447526328 val_score=0.8820732980080426
iter/secs=18.225   lr=0.01
epoch 4/10
loss=1.8229491710662842 train_score=0.8783590363963525 val_score=0.8575545201387538
iter/secs=0.3333333333333333   lr=0.01
loss=1.5297415256500244 train_score=0.8768593701923215 val_score=0.8592425494233535
iter/secs=18.692307692307693   lr=0.01
loss=1.397124171257019 train_score=0.8867157356139048 val_score=0.874094453283106
iter/secs=19.18421052631579   lr=0.01
loss=1.2251005172729492 train_score=0.9128207567057511 val_score=0.9101915751881818
iter/secs=19.18421052631579   lr=0.01
epoch 5/10
loss=1.6343339681625366 train_score=0.9151433952929187 val_score=0.9040891621202256
iter/secs=0.5   lr=0.01
loss=1.2310388088226318 train_score=0.9098265723174108 val_score=0.8833366187278981
iter/secs=19.7027027027027   lr=0.01
loss=1.1992015838623047 train_score=0.9090548357518391 val_score=0.8954125452664397
iter/secs=19.18421052631579   lr=0.01
loss=1.0464364290237427 train_score=0.9344759027727737 val_score=0.9071771008995814
iter/secs=19.7027027027027   lr=0.01
epoch 6/10
loss=0.9973875880241394 train_score=0.9192502086532521 val_score=0.8968029370685285
iter/secs=0.5   lr=0.01
loss=1.2285385131835938 train_score=0.9336282665672567 val_score=0.9146302358127705
iter/secs=19.18421052631579   lr=0.01
loss=1.4601562023162842 train_score=0.9313568267699648 val_score=0.9200092945547561
iter/secs=18.692307692307693   lr=0.01
loss=0.5415738821029663 train_score=0.9326309175283682 val_score=0.9035267138997524
iter/secs=19.18421052631579   lr=0.01
epoch 7/10
loss=1.3795852661132812 train_score=0.9402073743122974 val_score=0.9138010504596304
iter/secs=0.5   lr=0.01
loss=0.8704663515090942 train_score=0.9353219878565913 val_score=0.9162455313558932
iter/secs=19.18421052631579   lr=0.01
loss=1.2633082866668701 train_score=0.946238188860081 val_score=0.9118835435776022
iter/secs=19.18421052631579   lr=0.01
loss=1.2709053754806519 train_score=0.9426291262024048 val_score=0.9267987260074348
iter/secs=19.18421052631579   lr=0.01
epoch 8/10
loss=0.992490291595459 train_score=0.930934567137666 val_score=0.908815669652946
iter/secs=0.5   lr=0.01
loss=0.3549320101737976 train_score=0.9408060956734496 val_score=0.9229555924397368
iter/secs=19.18421052631579   lr=0.01
loss=1.1339396238327026 train_score=0.9475904546183302 val_score=0.915818535535917
iter/secs=18.692307692307693   lr=0.01
loss=1.0342168807983398 train_score=0.9499464962238433 val_score=0.9314815516529404
iter/secs=19.18421052631579   lr=0.01
epoch 9/10
loss=0.8813530206680298 train_score=0.942828199022009 val_score=0.9143749256669529
iter/secs=0.3333333333333333   lr=0.01
loss=1.1348878145217896 train_score=0.9499827220190885 val_score=0.9227171000061801
iter/secs=18.225   lr=0.01
loss=0.7621665000915527 train_score=0.9579630136227393 val_score=0.93287603788525
iter/secs=18.692307692307693   lr=0.01
loss=1.0629360675811768 train_score=0.9493479684055386 val_score=0.9237524258442978
iter/secs=18.225   lr=0.01
epoch 10/10
loss=0.8716220259666443 train_score=0.9572966199727819 val_score=0.9229388996621776
iter/secs=0.3333333333333333   lr=0.01
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


RESNET_KWARGS={'arch':'small_resnet18', 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
epoch 1/10
loss=9.708222389221191 train_score=0.061839574711407025 val_score=0.059792981491796376
iter/secs=0.1   lr=0.001
loss=5.3840203285217285 train_score=0.2754392934539294 val_score=0.2724926293735248
iter/secs=3.66   lr=0.001
loss=2.8890576362609863 train_score=0.5227201523829951 val_score=0.5036476022281131
iter/secs=4.101123595505618   lr=0.001
loss=1.8737618923187256 train_score=0.6900276684827787 val_score=0.6655098820835035
iter/secs=4.24031007751938   lr=0.001
epoch 2/10
loss=1.3181812763214111 train_score=0.7084501633217144 val_score=0.6819502036339755
iter/secs=4.104575163398692   lr=0.001
loss=1.3907277584075928 train_score=0.7896969922731925 val_score=0.7643320900986822
iter/secs=4.175257731958763   lr=0.001
loss=1.1355198621749878 train_score=0.7754518679894893 val_score=0.7449038152804595
iter/secs=4.239316239316239   lr=0.001
loss=1.0269896984100342 train_score=0.849998680162466 val_score=0.8217522378274746
iter/secs=4.3161764705882355   lr=0.001
epoch 3/10
loss=0.6764812469482422 train_score=0.851279977177622 val_score=0.8220570127725971
iter/secs=4.2687074829931975   lr=0.001
loss=0.6687164306640625 train_score=0.8620671506962153 val_score=0.8297356940190118
iter/secs=4.328313253012048   lr=0.001
loss=0.6667309403419495 train_score=0.8813684308134956 val_score=0.8479435190232056
iter/secs=4.375675675675676   lr=0.001
loss=1.173421859741211 train_score=0.8845305263613016 val_score=0.8494248202867176
iter/secs=4.41421568627451   lr=0.001
epoch 4/10
loss=0.5284425020217896 train_score=0.8878033572312495 val_score=0.8492629742262812
iter/secs=4.366589327146172   lr=0.001
loss=0.6225853562355042 train_score=0.9046096794781089 val_score=0.866651243839873
iter/secs=4.372881355932203   lr=0.001
loss=0.6555216312408447 train_score=0.8736687405979505 val_score=0.8332274000182063
iter/secs=4.403921568627451   lr=0.001
loss=0.5679194927215576 train_score=0.9115539470281799 val_score=0.8723849566472646
iter/secs=4.414545454545454   lr=0.001
epoch 5/10
loss=0.4460638761520386 train_score=0.9078178373076096 val_score=0.8644482317075016
iter/secs=4.386363636363637   lr=0.001
loss=0.44691044092178345 train_score=0.9187820226805944 val_score=0.8745006611136898
iter/secs=4.404255319148936   lr=0.001
loss=0.5019722580909729 train_score=0.924279713282093 val_score=0.8825577231311195
iter/secs=4.433641975308642   lr=0.001
loss=0.616363525390625 train_score=0.912863429398107 val_score=0.8696951778332053
iter/secs=4.440406976744186   lr=0.001
epoch 6/10
loss=0.4404523968696594 train_score=0.9121151859918966 val_score=0.8728274937292498
iter/secs=4.416901408450705   lr=0.001
loss=0.359135240316391 train_score=0.9318753217168085 val_score=0.8867057926061235
iter/secs=4.435828877005347   lr=0.001
loss=0.4580833315849304 train_score=0.9333767418570341 val_score=0.8897733464616295
iter/secs=4.447268106734435   lr=0.001
loss=0.2620128393173218 train_score=0.9286030856588402 val_score=0.8814500617597484
iter/secs=4.463030303030303   lr=0.001
epoch 7/10
loss=0.4659259021282196 train_score=0.9338599978939458 val_score=0.8858291416218472
iter/secs=4.4375   lr=0.001
loss=0.3760203421115875 train_score=0.939040813385583 val_score=0.8907781547718536
iter/secs=4.447576099210823   lr=0.001
loss=0.44573846459388733 train_score=0.9449985148057035 val_score=0.895326655119818
iter/secs=4.461621621621622   lr=0.001
loss=0.3679908514022827 train_score=0.9449114629677864 val_score=0.8966116382972071
iter/secs=4.469917012448133   lr=0.001
epoch 8/10
loss=0.23137331008911133 train_score=0.9485885269312135 val_score=0.8933250336659588
iter/secs=4.452332657200811   lr=0.001
loss=0.19283142685890198 train_score=0.9572680052466749 val_score=0.9011866619167688
iter/secs=4.46484375   lr=0.001
loss=0.249503955245018 train_score=0.942923920391337 val_score=0.8913783333111016
iter/secs=4.476459510357816   lr=0.001
loss=0.39736413955688477 train_score=0.949557912456267 val_score=0.9015259109402011
iter/secs=4.487272727272727   lr=0.001
epoch 9/10
loss=0.22272875905036926 train_score=0.9425487461191302 val_score=0.8907128681085786
iter/secs=4.471479500891266   lr=0.001
loss=0.22276201844215393 train_score=0.9559947741852667 val_score=0.9012426735559868
iter/secs=4.478036175710594   lr=0.001
loss=0.28975409269332886 train_score=0.9621646166637828 val_score=0.9084080923527905
iter/secs=4.480432972522897   lr=0.001
loss=0.26426583528518677 train_score=0.9523880565781151 val_score=0.894231585920817
iter/secs=4.482675261885576   lr=0.001
epoch 10/10
loss=0.21369636058807373 train_score=0.9539528883357413 val_score=0.899158786808987
iter/secs=4.4687252573238325   lr=0.001
loss=0.13797593116760254 train_score=0.969048945840657 val_score=0.9144638104894515
iter/secs=4.474654377880184   lr=0.001
loss=0.2174825370311737 train_score=0.9599210667021408 val_score=0.9028194479351724
iter/secs=4.486930545182973   lr=0.001
loss=0.24694743752479553 train_score=0.9587334170263951 val_score=0.9034571122366766
iter/secs=4.4920174165457185   lr=0.001
validation accuracy = [0.8882195  0.96808404 0.9666152 ]
score=0.9268647337293552
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]




RESNET_KWARGS={'arch':'small_resnext', 'groups': 32, 'width_per_group': 8, 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=64
IMG_H=64
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]




RESNET_KWARGS={'arch':'small_resnext', 'groups': 32, 'width_per_group': 8, 'block':Bottleneck, 'layers':[2, 2, 2, 2], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
epoch 1/10
loss=9.941219329833984 train_score=0.06141774891774891 val_score=0.06141290800936214
iter/secs=0.06666666666666667   lr=0.001
loss=5.163323879241943 train_score=0.28554383886132995 val_score=0.27756112215253964
iter/secs=2.472972972972973   lr=0.001
loss=2.64143967628479 train_score=0.586654937079808 val_score=0.5654808061237179
iter/secs=2.7238805970149254   lr=0.001
loss=1.7670882940292358 train_score=0.7192471854485878 val_score=0.6944287689005383
iter/secs=2.8195876288659796   lr=0.001
epoch 2/10
loss=1.5003399848937988 train_score=0.748826034216787 val_score=0.7202213296332703
iter/secs=2.7304347826086954   lr=0.001
loss=1.2476532459259033 train_score=0.8187196737976795 val_score=0.7886566737635203
iter/secs=2.783505154639175   lr=0.001
loss=1.0141780376434326 train_score=0.850669467159113 val_score=0.8149683471721096
iter/secs=2.834285714285714   lr=0.001
loss=0.962516188621521 train_score=0.8694696415530662 val_score=0.8385779271418661
iter/secs=2.8634146341463413   lr=0.001
epoch 3/10
loss=0.5444933176040649 train_score=0.883716270957377 val_score=0.8478413435332303
iter/secs=2.8202247191011236   lr=0.001
loss=0.6709549427032471 train_score=0.8699093105959993 val_score=0.8341993064319839
iter/secs=2.8511904761904763   lr=0.001
loss=0.6412215232849121 train_score=0.8801965422694817 val_score=0.8430661090932826
iter/secs=2.870567375886525   lr=0.001
loss=0.672969400882721 train_score=0.9030443960809352 val_score=0.871029332574897
iter/secs=2.8908507223113964   lr=0.001
epoch 4/10
loss=0.46966415643692017 train_score=0.9046989349310665 val_score=0.8666924564991739
iter/secs=2.8558421851289832   lr=0.001
loss=0.4161749482154846 train_score=0.8999463025053408 val_score=0.8552864003011883
iter/secs=2.870653685674548   lr=0.001
loss=0.3733445405960083 train_score=0.9047948150975196 val_score=0.8692877584656646
iter/secs=2.886889460154242   lr=0.001
loss=0.562938928604126 train_score=0.9089326971874656 val_score=0.8691989588253992
iter/secs=2.8973747016706444   lr=0.001
epoch 5/10
loss=0.3537939190864563 train_score=0.9154082236764659 val_score=0.8755991222354057
iter/secs=2.8739977090492554   lr=0.001
loss=0.31584978103637695 train_score=0.9359009260615142 val_score=0.8957724497036244
iter/secs=2.8811563169164884   lr=0.001
loss=0.47024062275886536 train_score=0.9329592982949266 val_score=0.8905776148315884
iter/secs=2.8874371859296484   lr=0.001
loss=0.3915577232837677 train_score=0.9202770662436563 val_score=0.8762432458841399
iter/secs=2.8929924242424243   lr=0.001
epoch 6/10
loss=0.34170329570770264 train_score=0.935526298212817 val_score=0.889987671004762
iter/secs=2.869167429094236   lr=0.001
loss=0.3541431427001953 train_score=0.9382802000290091 val_score=0.8991636439051142
iter/secs=2.8777103209019947   lr=0.001
loss=0.3510814309120178 train_score=0.9350736681264809 val_score=0.8936795940798914
iter/secs=2.8854080791426218   lr=0.001
loss=0.3854410946369171 train_score=0.9469863113529731 val_score=0.9044161359051077
iter/secs=2.89010989010989   lr=0.001
epoch 7/10
loss=0.22344966232776642 train_score=0.927371246160943 val_score=0.8819359565686529
iter/secs=2.874713521772345   lr=0.001
loss=0.3716158866882324 train_score=0.9522583828316282 val_score=0.9049615469413851
iter/secs=2.881665449233017   lr=0.001
loss=0.23549555242061615 train_score=0.9497207337213504 val_score=0.9035921894767892
iter/secs=2.888033589923023   lr=0.001
loss=0.29792970418930054 train_score=0.9616219732742952 val_score=0.9155110189562364
iter/secs=2.8958333333333335   lr=0.001
epoch 8/10
loss=0.31423068046569824 train_score=0.9439559186968008 val_score=0.905463532945276
iter/secs=2.8805774278215224   lr=0.001
loss=0.27658483386039734 train_score=0.9556437210951901 val_score=0.9090091163051317
iter/secs=2.8881869867340493   lr=0.001
loss=0.3926243782043457 train_score=0.9516685014053763 val_score=0.8986625499546947
iter/secs=2.893487522824102   lr=0.001
loss=0.262589693069458 train_score=0.947004829655071 val_score=0.9015770312948432
iter/secs=2.9001175088131608   lr=0.001
epoch 9/10
loss=0.2253822237253189 train_score=0.9382508100703908 val_score=0.889597008491471
iter/secs=2.886651323360184   lr=0.001
loss=0.12458699196577072 train_score=0.9569432333657347 val_score=0.9050980574782768
iter/secs=2.8931552587646077   lr=0.001
loss=0.18006697297096252 train_score=0.9612399489070956 val_score=0.9108110174103237
iter/secs=2.8992456896551726   lr=0.001
loss=0.3333892226219177 train_score=0.9593257820146088 val_score=0.9033779985624264
iter/secs=2.903444676409186   lr=0.001
epoch 10/10
loss=0.127410426735878 train_score=0.9580866138061339 val_score=0.9103573603459497
iter/secs=2.892875448487955   lr=0.001
loss=0.049634408205747604 train_score=0.9891386309142263 val_score=0.9377329425390897
iter/secs=2.8970661362506216   lr=0.0005
loss=0.051191747188568115 train_score=0.9938011529479105 val_score=0.9386111250173879
iter/secs=2.902415458937198   lr=0.0005
loss=0.05269074812531471 train_score=0.9927185338592512 val_score=0.9381080962921011
iter/secs=2.9061032863849765   lr=0.0005
validation accuracy = [0.9241934  0.9787393  0.97642404]
score=0.9488611748892675
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=1


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet

#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=1


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet

#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
epoch 1/10
loss=9.891732215881348 train_score=0.061258674050754816 val_score=0.061207399544470374
iter/secs=0.043478260869565216   lr=0.001
loss=4.845736503601074 train_score=0.30858973752845686 val_score=0.3020156816006553
iter/secs=1.5508474576271187   lr=0.001
loss=2.140641927719116 train_score=0.6304274948546308 val_score=0.6154329908050704
iter/secs=1.721698113207547   lr=0.001
loss=1.4303460121154785 train_score=0.7713308079421253 val_score=0.7473951249813724
iter/secs=1.7934426229508196   lr=0.001
epoch 2/10
loss=1.2541780471801758 train_score=0.7882569326275372 val_score=0.7690516916218246
iter/secs=1.7444444444444445   lr=0.001
loss=0.9772460460662842 train_score=0.8253508490486126 val_score=0.8065148013063377
iter/secs=1.7841409691629957   lr=0.001
loss=0.8694987893104553 train_score=0.8161006882505084 val_score=0.7901621550820142
iter/secs=1.8069216757741349   lr=0.001
loss=0.8787081837654114 train_score=0.8673406305342175 val_score=0.8412068380382055
iter/secs=1.8258164852255054   lr=0.001
epoch 3/10
loss=0.46194618940353394 train_score=0.8879933706578861 val_score=0.8601770298594946
iter/secs=1.7979942693409743   lr=0.001
loss=0.6255653500556946 train_score=0.8590470273726978 val_score=0.8367455570889092
iter/secs=1.8143939393939394   lr=0.001
loss=0.6678508520126343 train_score=0.8940601149375276 val_score=0.8662598294519519
iter/secs=1.827313769751693   lr=0.001
loss=0.569638192653656 train_score=0.8989046226212618 val_score=0.870118960320381
iter/secs=1.8377551020408163   lr=0.001
epoch 4/10
loss=0.4382385313510895 train_score=0.9169955134377095 val_score=0.8868718207174091
iter/secs=1.8183574879227054   lr=0.001
loss=0.36713194847106934 train_score=0.9091124019610486 val_score=0.8777057651618589
iter/secs=1.8281665190434013   lr=0.001
loss=0.350202351808548 train_score=0.9174941795052366 val_score=0.8852044005990375
iter/secs=1.8349673202614378   lr=0.001
loss=0.543990969657898 train_score=0.8968448742058026 val_score=0.8650168649349189
iter/secs=1.8435839028094154   lr=0.001
epoch 5/10
loss=0.34713712334632874 train_score=0.914502572216261 val_score=0.889047999000614
iter/secs=1.8287172011661808   lr=0.001
loss=0.33741313219070435 train_score=0.9050516952467824 val_score=0.8726925520079366
iter/secs=1.835607094133697   lr=0.001
loss=0.4916341304779053 train_score=0.9096938978580933 val_score=0.8808866662697883
iter/secs=1.840486867392697   lr=0.001
loss=0.3802592158317566 train_score=0.8903613813371188 val_score=0.8526680010263823
iter/secs=1.8459214501510575   lr=0.001
epoch 6/10
loss=0.2599369287490845 train_score=0.9249307676549494 val_score=0.8909468579401454
iter/secs=1.8339181286549708   lr=0.001
loss=0.3263370394706726 train_score=0.9428135299814375 val_score=0.9080503858411775
iter/secs=1.8392461197339247   lr=0.001
loss=0.35668057203292847 train_score=0.9321480220557044 val_score=0.8980968767256262
iter/secs=1.8440463645943097   lr=0.001
loss=0.42959731817245483 train_score=0.9408372300575818 val_score=0.9050762900299267
iter/secs=1.8483935742971886   lr=0.001
epoch 7/10
loss=0.2090260237455368 train_score=0.9367310650225601 val_score=0.9040691050153226
iter/secs=1.8382999511480216   lr=0.001
loss=0.33116161823272705 train_score=0.9506321273201465 val_score=0.912093718167327
iter/secs=1.8425969173283512   lr=0.001
loss=0.27031996846199036 train_score=0.952008336306909 val_score=0.9160059482627847
iter/secs=1.8473589973142346   lr=0.001
loss=0.28974366188049316 train_score=0.9666247132432604 val_score=0.9276045147888249
iter/secs=1.851740438332617   lr=0.001
epoch 8/10
loss=0.38599854707717896 train_score=0.9555459338844223 val_score=0.9156315954684999
iter/secs=1.8429890848026869   lr=0.001
loss=0.2787078619003296 train_score=0.9474853316483335 val_score=0.9108666488212117
iter/secs=1.8472727272727272   lr=0.001
loss=0.21149015426635742 train_score=0.9484188372615102 val_score=0.9038705431246342
iter/secs=1.850525496302063   lr=0.001
loss=0.40173786878585815 train_score=0.9330608306823003 val_score=0.8915194629572314
iter/secs=1.854244928625094   lr=0.001
epoch 9/10
loss=0.26021280884742737 train_score=0.9468951879247618 val_score=0.903096208149442
iter/secs=1.8472017673048602   lr=0.001
loss=0.24725747108459473 train_score=0.9645454452305653 val_score=0.9225524793467668
iter/secs=1.8508365966536133   lr=0.001
loss=0.26349544525146484 train_score=0.9620189897468863 val_score=0.9153644591603558
iter/secs=1.854238456237078   lr=0.001
loss=0.25417274236679077 train_score=0.9649455163595236 val_score=0.9142665857232608
iter/secs=1.8574290484140235   lr=0.001
epoch 10/10
loss=0.20265716314315796 train_score=0.9626522133944599 val_score=0.9181516512404232
iter/secs=1.8504918032786886   lr=0.001
loss=0.10426919162273407 train_score=0.9526250648865225 val_score=0.9038724403652352
iter/secs=1.8536430162265352   lr=0.001
loss=0.21737177670001984 train_score=0.9611793870611391 val_score=0.9162995217518246
iter/secs=1.8571870170015456   lr=0.001
loss=0.2601872682571411 train_score=0.9590859656382981 val_score=0.9165493878350137
iter/secs=1.8599759615384615   lr=0.001
validation accuracy = [0.9027335  0.97301334 0.9710217 ]
score=0.9367540820831031
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.4649019241333 val_score=0.06207188457727786
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.4649019241333 val_score=0.06207188457727786
iter/secs=0.0   lr=0.001
