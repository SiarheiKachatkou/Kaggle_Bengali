[00:13<17:25,  2.96batch/s
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.46450138092041 val_score=0.06298701298701298
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=512
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=512
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=512
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=512
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=512
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=512
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

USE_FREQ_SAMPLING=False

FAST_PROTO_SCALE=1

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.465625 val_score=0.060159968826969445
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.453215026855469 val_score=0.06427976409268554
iter/secs=0.0   lr=0.001
val_loss=1.2882592768380137 val_score=0.7997903770710661
iter/secs=14.180710843441561   lr=0.001
val_loss=1.0118851477259465 val_score=0.8587962319558294
iter/secs=14.220922886904004   lr=0.001
val_loss=0.6821382698354919 val_score=0.8979062843766554
iter/secs=14.180764254681254   lr=0.001
val_loss=0.6202728221195927 val_score=0.9083249192289173
iter/secs=14.14082515376739   lr=0.001
val_loss=0.556471734621118 val_score=0.9144216994150655
iter/secs=14.101107751564324   lr=0.001
val_loss=0.5374431142158675 val_score=0.9237200465824379
iter/secs=14.074753201165233   lr=0.001
val_loss=0.49845870526023267 val_score=0.9317293308796823
iter/secs=14.044752961828168   lr=0.001
val_loss=0.5260414094730997 val_score=0.9220429665664437
iter/secs=14.022336576580603   lr=0.001
val_loss=0.5315168307800042 val_score=0.9306628591856949
iter/secs=13.944435836768003   lr=0.001
val_loss=0.5470115401599396 val_score=0.9296478904895711
iter/secs=13.882735684327608   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.489961051940918 val_score=0.06490883059940372
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=0


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=0


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.489961051940918 val_score=0.06490883059940372
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.453215026855469 val_score=0.06427976409268554
iter/secs=0.0   lr=0.001
val_loss=1.2882592768380137 val_score=0.7997903770710661
iter/secs=13.944366975739023   lr=0.001
val_loss=1.0118851477259465 val_score=0.8587962319558294
iter/secs=14.022307200259219   lr=0.001
val_loss=0.6821382698354919 val_score=0.8979062843766554
iter/secs=13.996256512534364   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.453215026855469 val_score=0.06427976409268554
iter/secs=0.0   lr=0.001
val_loss=1.2882592768380137 val_score=0.7997903770710661
iter/secs=13.867326699852487   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.469573211669921 val_score=0.06893939393939394
iter/secs=0.0   lr=0.001
val_loss=1.2763750960952358 val_score=0.7847413434847041
iter/secs=9.616821391488923   lr=0.001
val_loss=0.7175278884657262 val_score=0.8855610521354697
iter/secs=9.691100982430536   lr=0.001
val_loss=0.5845719874618156 val_score=0.9134804156392531
iter/secs=9.716116495333555   lr=0.001
val_loss=0.5562311332357938 val_score=0.9178574223263296
iter/secs=9.644563261706761   lr=0.001
val_loss=0.5673075232720166 val_score=0.914186477181451
iter/secs=9.52922586998795   lr=0.001
val_loss=0.514071014960797 val_score=0.9330063025561611
iter/secs=9.543720187756534   lr=0.001
val_loss=0.5085388746890915 val_score=0.9313430505003646
iter/secs=9.569711563337929   lr=0.001
val_loss=0.488118395138015 val_score=0.9367515526448458
iter/secs=9.58929819040201   lr=0.001
val_loss=0.5093190781118577 val_score=0.9351455448419078
iter/secs=9.60050590713731   lr=0.001
val_loss=0.5042823470380318 val_score=0.939210662816251
iter/secs=9.598466692746962   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=1
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.469573211669921 val_score=0.06893939393939394
iter/secs=0.0   lr=0.001
val_loss=1.2763750960952358 val_score=0.7847413434847041
iter/secs=9.691082273813615   lr=0.001
validation accuracy = [0.75756824 0.9292223  0.93333   ]
score=0.8701338849220551
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.469573211669921 val_score=0.06893939393939394
iter/secs=0.0   lr=0.001
val_loss=1.287935468853946 val_score=0.784847529398091
iter/secs=9.691082273813615   lr=0.001
val_loss=0.8087674376496858 val_score=0.8707672365367914
iter/secs=9.80466835025713   lr=0.001
val_loss=0.6197913561188624 val_score=0.9065742906853369
iter/secs=9.830274373009956   lr=0.001
val_loss=0.5000149765938663 val_score=0.923205947747745
iter/secs=9.83348694075716   lr=0.001
val_loss=0.49202798150087657 val_score=0.9260093178875065
iter/secs=9.827713525674609   lr=0.001
val_loss=0.43183327346232137 val_score=0.9364105413203634
iter/secs=9.823868347117843   lr=0.001
val_loss=0.429981799965555 val_score=0.9391887623333399
iter/secs=9.815636974504484   lr=0.001
val_loss=0.40006477749376207 val_score=0.9422346437735764
iter/secs=9.823869949182999   lr=0.001
val_loss=0.3782501266987035 val_score=0.9428154962372175
iter/secs=9.821734860115278   lr=0.001
val_loss=0.36962487472539124 val_score=0.9469276560711046
iter/secs=9.827717373642374   lr=0.001
validation accuracy = [0.9328072 0.9807558 0.9815525]
loaded model score=0.9546202063041831
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=2


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=2


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=800
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.475322914123534 val_score=0.027000777000777
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=32
EPOCHS=30
LR=0.001
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=3


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=32
EPOCHS=30
LR=0.001
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi

#resnext
#RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

#resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnet
#RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


#se-resnext
RESNET_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}


SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.460528373718262 val_score=0.0286096256684492
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=32
EPOCHS=30
LR=0.001
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(_m*3,_m*6,_m*4), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=32
EPOCHS=30
LR=0.001
LR_SCHEDULER_PATINCE=8
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(_m*3,_m*6,_m*4), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(_m*3,_m*6,_m*4), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(_m*3,_m*6,_m*4), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(_m*3,_m*6,_m*4), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(_m*3,_m*6,_m*4), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':int(0.3*beta**phi), 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.48543586730957 val_score=0.06275190326914465
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.48543586730957 val_score=0.06275190326914465
iter/secs=0.0   lr=0.001
val_loss=1.2454724791897922 val_score=0.7968851035418735
iter/secs=12.741052075877787   lr=0.001
val_loss=0.840777859067993 val_score=0.8657747172747566
iter/secs=12.741084413491338   lr=0.001
val_loss=0.6743526598483182 val_score=0.8984776077946868
iter/secs=12.762690232728419   lr=0.001
val_loss=0.5988424630446487 val_score=0.9082767196315219
iter/secs=12.806106114660569   lr=0.001
val_loss=0.5528673309078247 val_score=0.9168149190587953
iter/secs=12.80610938152104   lr=0.001
val_loss=0.5130754147182431 val_score=0.9251319378687639
iter/secs=12.817010368501814   lr=0.001
val_loss=0.5037025536218899 val_score=0.9256499876458609
iter/secs=12.796786018364154   lr=0.001
val_loss=0.4683392302889193 val_score=0.9329656303003252
iter/secs=12.789800770827535   lr=0.001
val_loss=0.46197631252867183 val_score=0.9344414845068512
iter/secs=12.784373070530238   lr=0.001
val_loss=0.47714664536324797 val_score=0.9312943907476792
iter/secs=12.773530395149926   lr=0.001
validation accuracy = [0.91577876 0.97231627 0.9717188 ]
loaded model score=0.9397075774669013
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=1


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=1


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONRESNETV2_KWARGS
BACKBONE_FN=InceptionResNetV2

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.454801177978515 val_score=0.06294606294606295
iter/secs=0.0   lr=0.001
val_loss=1.2887721230919091 val_score=0.7916138804711162
iter/secs=11.4090390498225   lr=0.001
val_loss=0.8153653430976746 val_score=0.8692598978704538
iter/secs=11.409064979397774   lr=0.001
val_loss=0.6558098546340705 val_score=0.9000612666487231
iter/secs=11.409073622615724   lr=0.001
val_loss=0.621047728844997 val_score=0.913270310441977
iter/secs=11.422057540321342   lr=0.001
val_loss=0.5171878289853177 val_score=0.9236241441869377
iter/secs=11.419461856722606   lr=0.001
val_loss=0.4857062588563186 val_score=0.926702922462548
iter/secs=11.41773205630625   lr=0.001
val_loss=0.47219311737082603 val_score=0.928773767526162
iter/secs=11.43135235435761   lr=0.001
val_loss=0.457473273149898 val_score=0.9332274494497517
iter/secs=11.435073214650789   lr=0.001
val_loss=0.45448602181064646 val_score=0.9347536896031476
iter/secs=11.43796889216765   lr=0.001
val_loss=0.4277201982087306 val_score=0.9381913732655593
iter/secs=11.42466480443113   lr=0.001
validation accuracy = [0.9234963 0.9763991 0.9761253]
loaded model score=0.9467813279225349
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4

SEED=0

TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

try:
    import torch
    torch.manual_seed(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
except:
    print('can not import pytorch, gpu use will be undeterministic')


np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.44737377166748 val_score=0.05844155844155844
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=32
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=32
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.449110984802246 val_score=0.025
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.45058650970459 val_score=0.025
iter/secs=0.0   lr=0.001
val_loss=6.919617039463909 val_score=0.1354375943365498
iter/secs=11.308545823709657   lr=0.001
val_loss=5.321588298144094 val_score=0.2651338813271271
iter/secs=11.346886244697037   lr=0.001
val_loss=4.275606490036406 val_score=0.3656853378804964
iter/secs=11.334081514642017   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.6*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.6*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.44716682434082 val_score=0.07500000000000001
iter/secs=0.0   lr=0.001
val_loss=8.01663867885848 val_score=0.061425168616344644
iter/secs=10.22605883293398   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=32
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=32
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=INCEPTIONV4_KWARGS
BACKBONE_FN=InceptionV4


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=9.473205375671387 val_score=0.07272727272727272
iter/secs=0.0   lr=0.001
val_loss=2.331440660488083 val_score=0.6231276354386139
iter/secs=15.261351789204289   lr=0.001
val_loss=1.051495652915947 val_score=0.8313519875855835
iter/secs=15.307903494049553   lr=0.001
val_loss=0.8723326864233055 val_score=0.8696108049908633
iter/secs=15.307911272447894   lr=0.001
val_loss=0.6779137023891586 val_score=0.8989543263308977
iter/secs=15.331286006651904   lr=0.001
val_loss=0.5929688374003567 val_score=0.913158473361432
iter/secs=15.33597108370734   lr=0.001
val_loss=0.5638597814329117 val_score=0.912044651440662
iter/secs=15.331289907740505   lr=0.001
val_loss=0.5296987887160237 val_score=0.9173440504512334
iter/secs=15.348028232302081   lr=0.001
val_loss=0.524488499903109 val_score=0.9254724901133048
iter/secs=15.395931239581738   lr=0.001
val_loss=0.5067763302846259 val_score=0.9277373643594367
iter/secs=15.42286163042265   lr=0.001
val_loss=0.48009931202310013 val_score=0.9303144326031851
iter/secs=15.449226015622763   lr=0.001
validation accuracy = [0.9136875  0.9748556  0.97256523]
loaded model score=0.940337751003542
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=32
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(84*gama**phi)
IMG_H=int(84*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=32
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=32
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=32
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.5
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=8.606603050231934 val_score=0.05506318006318006
iter/secs=0.0   lr=0.001
val_loss=0.9715088241313253 val_score=0.7316580221041467
iter/secs=16.84552452668103   lr=0.001
val_loss=0.5731725796130285 val_score=0.8460690868086718
iter/secs=16.90229999225592   lr=0.001
val_loss=0.48266897365997474 val_score=0.8662754049803096
iter/secs=16.883370216658708   lr=0.001
val_loss=0.35834200759871343 val_score=0.8938457292880341
iter/secs=16.90232844725851   lr=0.001
val_loss=0.32223647404491806 val_score=0.9087348926845176
iter/secs=16.890959769906097   lr=0.001
val_loss=0.2869175275879804 val_score=0.9112710079864046
iter/secs=16.80801695533822   lr=0.001
val_loss=0.26990419231129414 val_score=0.9208569008667719
iter/secs=16.82948579551169   lr=0.001
val_loss=0.2360294521169847 val_score=0.9305607857366103
iter/secs=16.845623451658177   lr=0.001
val_loss=0.2224352833246025 val_score=0.9344491463816821
iter/secs=16.85819637448032   lr=0.001
val_loss=0.23302479317052321 val_score=0.9313485865927256
iter/secs=16.868268233690703   lr=0.001
validation accuracy = [0.914285  0.9740589 0.9740341]
loaded model score=0.9391157800813205
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.9
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.9
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=8.606603050231934 val_score=0.05506318006318006
iter/secs=0.0   lr=0.001
val_loss=1.0749246400128902 val_score=0.709293782083955
iter/secs=16.733221778521475   lr=0.001
val_loss=0.6993148736120981 val_score=0.8139208680371046
iter/secs=16.789241507553488   lr=0.001
val_loss=0.47263705861150174 val_score=0.8713208793207508
iter/secs=16.69619357828475   lr=0.001
val_loss=0.3498269826577801 val_score=0.9016313512225002
iter/secs=16.650055306707618   lr=0.001
val_loss=0.33236224302929934 val_score=0.9055315290709351
iter/secs=16.688807594670752   lr=0.001
val_loss=0.29101769895882507 val_score=0.9143398042649218
iter/secs=16.659273606998223   lr=0.001
val_loss=0.2573930497683787 val_score=0.9273251276174832
iter/secs=16.606789596607186   lr=0.001
val_loss=0.24813010500212226 val_score=0.9328079857305869
iter/secs=16.581324045149426   lr=0.001
val_loss=0.2265641252425584 val_score=0.9320717760534213
iter/secs=16.610281904204484   lr=0.001
val_loss=0.21650069674545308 val_score=0.9363593075150254
iter/secs=16.655596114402048   lr=0.001
validation accuracy = [0.9169488  0.975229   0.97231627]
loaded model score=0.9369781437586056
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.9
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.9
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=8.611854743957519 val_score=0.06435880338197271
iter/secs=0.0   lr=0.001
val_loss=1.9433850347995758 val_score=0.5765855843801175
iter/secs=5.598164302104445   lr=0.001
val_loss=0.9989093740781149 val_score=0.758728166996065
iter/secs=5.623293169088928   lr=0.001
val_loss=0.6607768033177425 val_score=0.8266034507754113
iter/secs=5.614908612213099   lr=0.001
val_loss=0.5528037706628824 val_score=0.8414237447731217
iter/secs=5.598201789728148   lr=0.001
val_loss=0.47885960531540406 val_score=0.869206375520535
iter/secs=5.58822533293167   lr=0.001
val_loss=0.4133761284443048 val_score=0.8724403567065304
iter/secs=5.589887682187694   lr=0.001
val_loss=0.34022578386924207 val_score=0.9025244923170963
iter/secs=5.598207145143948   lr=0.001
val_loss=0.3439116185674301 val_score=0.9014897548566938
iter/secs=5.610731979047004   lr=0.001
val_loss=0.3139791305248554 val_score=0.9070220395711392
iter/secs=5.614919786149467   lr=0.001
val_loss=0.32274841144680977 val_score=0.911650902267019
iter/secs=5.6233133423198725   lr=0.001
validation accuracy = [0.8843607  0.963329   0.96138716]
loaded model score=0.9153928331167915
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.9
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.9
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=8.606603050231934 val_score=0.05506318006318006
iter/secs=0.0   lr=0.001
val_loss=0.8082024491669839 val_score=0.7769710432658201
iter/secs=16.51304925625489   lr=0.001
val_loss=0.5404679702277009 val_score=0.853863113812432
iter/secs=16.567602087121827   lr=0.001
val_loss=0.41292215924133713 val_score=0.8827093608073129
iter/secs=16.5494141771117   lr=0.001
val_loss=0.3368916631077655 val_score=0.9067633172008116
iter/secs=16.56762942635408   lr=0.001
val_loss=0.29597209953662884 val_score=0.9124498654491608
iter/secs=16.62249453974233   lr=0.001
val_loss=0.26093017691345305 val_score=0.9223347230320363
iter/secs=16.640865590203767   lr=0.001
val_loss=0.2481896148332093 val_score=0.9270001617813463
iter/secs=16.638241819846762   lr=0.001
val_loss=0.2489224929196175 val_score=0.9308792223546256
iter/secs=16.636274534983816   lr=0.001
val_loss=0.2272216545646651 val_score=0.9320266882078756
iter/secs=16.647003207808986   lr=0.001
val_loss=0.23388037414095428 val_score=0.9311892047527661
iter/secs=16.655596114402048   lr=0.001
validation accuracy = [0.91296554 0.9742581  0.9643995 ]
loaded model score=0.9326135428478872
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.9
DROPOUT_P=0

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0.9
DROPOUT_P=0

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=8.606603050231934 val_score=0.05506318006318006
iter/secs=0.0   lr=0.001
val_loss=1.0749246400128902 val_score=0.709293782083955
iter/secs=16.62240647412931   lr=0.001
val_loss=0.6993148736120981 val_score=0.8139208680371046
iter/secs=16.6776854561945   lr=0.001
val_loss=0.47263705861150174 val_score=0.8713208793207508
iter/secs=16.69619357828475   lr=0.001
val_loss=0.3498269826577801 val_score=0.9016313512225002
iter/secs=16.733305444490927   lr=0.001
val_loss=0.33236224302929934 val_score=0.9055315290709351
iter/secs=16.71102967905502   lr=0.001
val_loss=0.29101769895882507 val_score=0.9143398042649218
iter/secs=16.7333147407614   lr=0.001
val_loss=0.2573930497683787 val_score=0.9273251276174832
iter/secs=16.76525117819544   lr=0.001
val_loss=0.24813010500212226 val_score=0.9328079857305869
iter/secs=16.775257497696327   lr=0.001
val_loss=0.2265641252425584 val_score=0.9320717760534213
iter/secs=16.795526546076918   lr=0.001
val_loss=0.21650069674545308 val_score=0.9363593075150254
iter/secs=16.778063651026972   lr=0.001
validation accuracy = [0.9169488  0.975229   0.97231627]
loaded model score=0.9369781437586056
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=2


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=2


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=8.649489974975586 val_score=0.07692895339954164
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4

ARTIFACTS_DIR='artifacts'
DATA_DIR='data'
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.1
gama=1.15
phi=0


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

BACKBONE_KWARGS=RESNET_KWARGS
BACKBONE_FN=_resnet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=8.606603050231934 val_score=0.05506318006318006
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=1 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=1 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=1 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=1 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0.5 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0.5 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

block_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'block_args':block_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=16
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(140*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(140*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=N_CHANNELS,
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 1 GPUs!
val_loss=8.485305786132812 val_score=0.03125
iter/secs=0.0   lr=0.001
val_loss=2.1099776069414236 val_score=0.5512431982461216
iter/secs=18.408790830068362   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

17 train images loaded
5 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

17 train images loaded
5 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=4
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 2 GPUs!
val_loss=8.483767318725587 val_score=0.03571428571428571
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=8
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(224*gama**phi)
IMG_H=int(224*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=8
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(112*gama**phi)
IMG_H=int(112*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=8
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(112*gama**phi)
IMG_H=int(112*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=8
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 2 GPUs!
val_loss=8.485363388061524 val_score=0.027777777777777776
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=8
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=8
EPOCHS=10
LR=0.001
LR_SCHEDULER_PATINCE=8000
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 2 GPUs!
val_loss=8.485363388061524 val_score=0.027777777777777776
iter/secs=0.0   lr=0.001
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=100
LR=0.01
LR_SCHEDULER_PATINCE=80
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=64
EPOCHS=100
LR=0.01
LR_SCHEDULER_PATINCE=80
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 2 GPUs!
val_loss=8.484968376159667 val_score=0.05844155844155844
iter/secs=0.0   lr=0.01
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=100
LR=0.05
LR_SCHEDULER_PATINCE=80
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=100
LR=0.05
LR_SCHEDULER_PATINCE=80
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 2 GPUs!
val_loss=8.48546905517578 val_score=0.061586212529608754
iter/secs=0.0   lr=0.05
val_loss=9780.46024263822 val_score=0.08928721025932251
iter/secs=7.646248216485164   lr=0.05
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=512
EPOCHS=100
LR=0.05
LR_SCHEDULER_PATINCE=80
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=512
EPOCHS=100
LR=0.05
LR_SCHEDULER_PATINCE=80
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 2 GPUs!
val_loss=8.486122322082519 val_score=0.06147186147186147
iter/secs=0.0   lr=0.05
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=100
LR=0.05
LR_SCHEDULER_PATINCE=80
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=100
LR=0.05
LR_SCHEDULER_PATINCE=80
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 2 GPUs!
val_loss=8.48546905517578 val_score=0.061586212529608754
iter/secs=0.0   lr=0.05
import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # phi=1 efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=100
LR=0.05
LR_SCHEDULER_PATINCE=80
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

import logging
import os
import numpy as np
import random
from .resnet import Bottleneck,SEBottleneck,_resnet
from .inceptionresnetv2 import InceptionResNetV2
from .inceptionv4 import InceptionV4
from .efficientnet import EfficientNet
from .efficientnet_utils import BlockDecoder, GlobalParams

ARTIFACTS_DIR='artifacts'
DATA_DIR=os.path.join(os.path.dirname(__file__),'../..','data')
RAW_DIR='raw'
MODELS_DIR='models'
MODELS_PRETRAINED_DIR='models_pretrained'
SUBMISSION_DIR = 'submissions'
SUBMISSION_CSV = 'submission.csv'
SAMPLE_SUBMISSION_CSV = 'sample_submission.csv'

LOG_FILENAME=os.path.join(os.path.dirname(__file__),'training_log.txt')

LOG_LEVEL=logging.INFO

METRIC_FILE_PATH='metric.txt'

TRAIN_IMAGE_DATA_PATTERN='train_image_data_*.parquet'
TEST_IMAGE_DATA_PATTERN='test_image_data_*.parquet'
TRAIN_CSV='train.csv'
TEST_CSV='test.csv'
CLASS_MAP_CSV='class_map.csv'

TRAIN_DATASET_DIR='train_datset'
VAL_DATASET_DIR='val_datset'
TEST_DATASET_DIR='test_datset'
IMAGE_GEN_PKL = 'image_gen.pkl'

MODEL_NAME='model'

alpha=1.2
beta=1.32
gama=1.28
phi=0 # phi=1 efficient net b3, phi=4 for b7


IMG_WIDTH = 236
IMG_HEIGHT = 137
IMG_W=int(64*gama**phi)
IMG_H=int(64*gama**phi)

DO_CROP_SYMBOL=False
TOP_CUT=4
LEFT_CUT=4
PAD=4


N_CHANNELS = 1
BATCH_SIZE=256
EPOCHS=100
LR=0.05
LR_SCHEDULER_PATINCE=80
AUGM_PROB=0
DROPOUT_P=0.5

LOSS_WEIGHTS=[1,1,1]
BETA=0

CLASSES_LIST=[168,11,7]


_m=alpha**phi


RESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

RESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':Bottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNET_KWARGS={'arch':'small_resnet', 'width_per_group': int(64*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

SERESNEXT_KWARGS={'arch':'small_resnet', 'groups': 32,'width_per_group': int(8*beta**phi), 'block':SEBottleneck, 'layers':[int(_m*2), int(_m*2), int(_m*2), int(_m*2)], 'num_classes':np.sum(CLASSES_LIST),'pretrained':False, 'progress':False}

INCEPTIONRESNETV2_KWARGS={'repeats':(int(_m*3),int(_m*6),int(_m*4)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

INCEPTIONV4_KWARGS={'repeats':(int(_m*2),int(_m*4),int(_m*2)), 'width':0.3*beta**phi, 'num_classes':np.sum(CLASSES_LIST)}

blocks_args = [
        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
        'r1_k3_s11_e6_i192_o320_se0.25',
    ]

blocks_args=BlockDecoder.decode(blocks_args)

global_params = GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=0.2,
        drop_connect_rate=0.2,
        num_classes=np.sum(CLASSES_LIST),
        width_coefficient=alpha**phi,
        depth_coefficient=beta**phi,
        depth_divisor=8,
        min_depth=None,
        image_size=IMG_W,
    )

EFFICIENTNET_KWARGS={'blocks_args':blocks_args, 'global_params':global_params}

BACKBONE_KWARGS=EFFICIENTNET_KWARGS
BACKBONE_FN=EfficientNet


TARGETS=['grapheme_root','vowel_diacritic','consonant_diacritic']

SEED=0

import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(SEED)
random.seed(SEED)

160672 train images loaded
40168 val images loaded
Let's use 2 GPUs!
val_loss=8.48546905517578 val_score=0.061586212529608754
iter/secs=0.0   lr=0.05
val_loss=9780.46024263822 val_score=0.08928721025932251
iter/secs=8.142751392839054   lr=0.05
val_loss=10.896851062774658 val_score=0.28871618831166884
iter/secs=8.090270385352353   lr=0.05
val_loss=31426.935997596152 val_score=0.06141774891774891
iter/secs=8.038427186208606   lr=0.05
val_loss=9.044159189248697 val_score=0.2841350467946158
iter/secs=8.012753952862772   lr=0.05
val_loss=3.63568032399202 val_score=0.49630385669720856
iter/secs=7.997428577988322   lr=0.05
val_loss=8.84709366162618 val_score=0.1907610820126293
iter/secs=8.004238288854705   lr=0.05
val_loss=1.2694399505853653 val_score=0.7270707972505281
iter/secs=7.994520957156727   lr=0.05
val_loss=1.3874680461027684 val_score=0.7171771531679003
iter/secs=7.987248427948363   lr=0.05
val_loss=3.8299385034121 val_score=0.49238481962282105
iter/secs=7.9816011575655486   lr=0.05
val_loss=3.4079225002191005 val_score=0.42300693484657365
iter/secs=7.977089087672916   lr=0.05
